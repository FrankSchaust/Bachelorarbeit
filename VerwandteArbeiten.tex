\section{Verwandte Arbeiten}
\label{VerwandteArbeiten}

Die Computerspiele StarCraft und StarCraft II waren schon mehrfach Anschauungsobjekt im Bezug auf Schlachtensimulation und der Vorhersage von Gefechtsausgängen. \textcite{AAAI:aimag/Robertson14} fassen Literatur zum Thema StarCraft zusammen und arbeiten Forschungsfelder oder Aufgaben heraus, welche einen positiven Einfluss auf die Forschung im Bereich der künstlichen Intelligenz (KI) haben könnten. Sie kommen zu dem Schluss, dass Machine Learning bei der Verbesserung bestehender KIs an Einfluss gewinnen wird und dass ein klarer Trend zu Machine-Learning-Verfahren zu erkennen ist. 

Verwandte Arbeiten existieren sowohl für die Spielumgebung von StarCraft \parencite{AIIDE137381, DBLP:conf/aiide/ChurchillSB12, AIIDE1511531, SnchezRuizGranados2015PredictingTO, 6633643, 6374183}, als auch für die Spielumgebung von StarCraft~II \parencite{DBLP:journals/corr/HelmkeKW14, samvelyan2019starcraft}. Die Arbeiten, die sich mit der Vorhersage von Gefechts-Ausgängen beschäftigen nutzen vor allem simulationsbasierte Ansätze  \parencite{6633643, DBLP:conf/aiide/ChurchillSB12, DBLP:journals/corr/HelmkeKW14} um optimale Handlungsfolgen vorherzusagen und diese im Wettkampf gegen existierende Skript zu testen, oder verwenden Machine Learning um die Ergebnisse der Gefechte vorherzusagen \parencite{SnchezRuizGranados2015PredictingTO, AIIDE137381, AIIDE1511531}.


\textcite{AIIDE137381} stellt ein Modell vor, auf dessen Basis ein Lernalgorithmus die offensiven und defensiven Feature Values (zu Deutsch: \textit{Eigenschaftswerte}) jeder Einheit erlernt. Feature Values können zum Beispiel in der Offensive der Schaden einer Einheit und in der Defensive die Lebenspunkte sein. Das Paper nennt als weiteres Beispiel noch Angriffsreichweite als offensives und Bewegungsrate als defensives Feature. Diese Feature Values werden nachfolgend für jede Einheit der Spieler berechnet. Anschließend werden die Feature Values beider Spieler aggregiert:

\begin{equation}
Agg = (O_A / D_B - O_B / D_A)
\end{equation}

Mit O als offensiven und D als defensiven Feature Value. Durch eine Sigmoid-Funktion wird anschließend die Wahrscheinlichkeit, das Spieler A Spieler B schlägt bestimmt. Da sich die Arbeit auf die Zusammensetzung von Armeen beschränkt, vernachlässigt es einige Faktoren, welche bei einer ganzheitlichen Betrachtung von Gefechten eine Rolle spielen. So wird Terrain und die Vor- und Nachteile die es mit sich bringt, außer Acht gelassen, fliegende Einheiten, sowie Einheiten, die Fähigkeiten nutzen werden ignoriert und Einheitenverbesserungen sind in dem Modell auch nicht existent. Zudem differenziert die Methode nicht nach Sieg, Niederlage oder Remis, sondern nur nach Sieg oder Niederlage.

Stanescu et al. \textcite{AIIDE1511531} bauen ihren Vorhersage-Algorithmus auf dem Gesetz von Lanchester (auch Lanchester's Square Law genannt) auf, welches er in seinem Buch \textit{ Aircraft in Warfare – The Dawn of the Fourth Arm} von 1916 formuliert. Hierbei handelt es sich um ein Modell, welches versucht die Verluste in militärischen Gefechtssituationen  einzuschätzen. Basierend auf der Einheitenzusammensetzung wird die \textit{Armee-Effektivität} $\alpha_{avg}$ für beide Spieler errechnet. In der Gleichung wird der Wert für Armee A berechnet. A bezieht sich auf die Anzahl der Einheiten und $\alpha_i$ ist die Effektivität einer Einheit und zeitgleich die lernbare Variable, die im Verlauf des Trainings immer weiter angepasst wird. $\alpha_i$ wird für jede Einheit zu Beginn des Trainings mit dem Produkt aus Schaden und Lebenspunkten der Einheit initialisiert.

\begin{equation}
	\alpha_{avg} = \frac{\sum_{i=1}^A alpha_i}{A}
\end{equation}

Der Algorithmus ersetzt unter Turnierbedingungen den simulationsbasierten Entscheidungsalgorithmus des UAlbertaBots im Bezug auf die Entscheidung ein Gefecht einzugehen oder sich zurückzuziehen. Das Modell zieht Interaktionen zwischen Einheiten, wie z.B. das Heilen durch Medics nicht in Betracht und ignoriert Einheitenpositionierung.  

\textcite{SnchezRuizGranados2015PredictingTO} beschränkt sich auf die Vorhersage der Gefechte und vergleicht die Präzision verschiedenener Machine-Learning-Algorithmen. In den Experimenten kommen die Klassifizierungs-Verfahren Linear Discriminant Analysis (LDA, zu Deutsch: \textit{Lineare Diskriminanzanalyse}), Quadratic Discriminant Analysis (QDA, zu Deutsch: \textit{Quadratische Diskriminanzanalyse}) Support Vector Machines (SVM, zu Deutsch etwa: \textit{Stützvektormaschine}), k-Nearest Neighbour (kNN, zu Deutsch: \textit{k nächste Nachbarn}) und Weighted k-Nearest Neighbour (kkNN, zu Deutsch: \textit{gewichtete k nächste Nachbarn}) zur Anwendung. Es werden im Allgemeinen vier Feature (zu Deutsch: \textit{Merkmale}) zur Evaluierung definiert: Die Anzahl der Einheiten, ihr durchschnittliches Leben und ihre relative Position, sowie Distanz zur gegnerischen Armee. Die relative Position wird angenähert, indem um jede Armee ein minimales Rechteck gezogen wird, dessen Fläche stellvertretend für die Streuung der Einheiten steht. Die Distanz zur feindlichen Armee wird bestimmt durch die Entfernung der Mittelpunkte beider Rechtecke. Die Vordefinierten Features beziehen sich nicht auf Merkmale der einzelnen Einheiten, sondern stellen die Armeen in den Fokus. Es werden daher einheitenspezifische Merkmale wie Schadenswerte der Einheiten, Rüstung oder Reichweite ignoriert.

Alle drei Arbeiten erreichen mit ihren Algorithmen die 90\%-Marke in der Vorhersage von Gefechtsausgängen, nutzen die Ergebnisse allerdings für unterschiedliche Weiterverwendungen. \textcite{AIIDE137381} versucht weiterführend für jede Einheitenzusammensetzung aus 2 unterschiedlichen Typen, die Armee zu finden, welche diese Zusammensetzung schlägt. \textcite{AIIDE1511531} vergleicht das Entscheidungsverhalten des Algorithmus mit gängigen Implementationen in StarCraft-Bots und \textcite{SnchezRuizGranados2015PredictingTO} evaluiert die Änderung der Accuracy im Verlauf des Gefechts und erreicht zwar mit allen vorgestellten Klassifizierungsverfahren die 90\%-Marke, jedoch lediglich kkNN liegt auch kurz nach Beginn des Gefechtes bei 90\%, während die restlichen Verfahren erst nach ca. 60\% der Gefechts-Zeit eine Accuracy von 90\% erreichen. Für einen Entscheidungsalgorithmus wäre die Arbeit von \textcite{AIIDE1511531} die beste Wahl, weil sie als einziges bereits geschädigte Einheiten berücksichtigen kann und mit der vorgestellten Methode eine Accuracy von 93\% erreicht. 

\textcite{DBLP:conf/aiide/ChurchillSB12} entwickeln einen Such-Algorithmus, der basierend auf einen Spiel-Zustand optimale Entscheidungen für eine KI treffen soll. Da die Suche nach einer optimalen Entscheidung oftmals langwierig sein kann, wird die maximale Dauer der Evaluierung in der aktiven Anwendung in einer KI auf 5ms beschränkt. Trotz dieser Einschränkung schafft es der Algorithmus einfache KI-Skripte zu schlagen. Das Modell des Algorithmus modelliert den Zustand des Spiels und jeder einzelnen Einheit und errechnet für jeden Zeitschritt eine Menge an Legal Moves (zu Deutsch: \textit{legale/erlaubte Züge}). Das Modell unterliegt einigen Limitierungen, die seine Nutzbarkeit einschränken. So werden Lebenspunkt-Regeneration, Einheitenkollision und Reisezeit von Projektilen nicht in die Evaluierung einbezogen. 

Portfolio greedy search \parencite{6633643} ist ein Such-Algorithmus, der -- ähnlich wie \textcite{DBLP:conf/aiide/ChurchillSB12} -- versucht Handlungsfolgen zu konstruieren, welche auf Gefechts-Szenarien mit bis zu 50 Einheiten pro Seite angewandt werden können. Im Zuge der Entwicklung konnten Churchill und Buro ebenfalls SparCraft vorstellen. Ein System mit dem Gefechte abstrakt modelliert werden können. 

Wender und Watson haben vorgestellt, wie Reinforcment Learning (zu Deutsch: \textit{Bestärkendes Lernen}) in StarCraft Anwendung finden kann \parencite{6374183}. Der von ihnen vorgestellte Algorithmus sollte das Mikromanagement der Einheiten erlernen und schlussendlich geskriptete Abfolgen ablösen. In ihrem Paper zeigten sie, dass Reinforcement Learning ein geeignetes Verfahren für diese Domäne ist. 

\textcite{DBLP:journals/corr/HelmkeKW14} entwickelt Näherungsmodelle basierend auf eingespeisten Grundkonstellationen für das Spiel StarCraft II. In dem Paper werden 4 Annäherungsmethoden vorgestellt, welche unterschiedliche Eigenschaften der beiden Armeen in Betracht ziehen. Es werden Lebenspunkte, Schaden pro Sekunde, Fernkampfschaden, Rüstung, Attribute, Boni und Bonus Schaden pro Sekunde als Eigenschaften einer Einheit festgelegt und in unterschiedlichen Zusammensetzungen gemeinsam evaluiert. Einheiten-Fähigkeiten, sowie Einheitenverbesserungen vernachlässigt werden vernachlässigt. Da einige Einheiten nur Einheiten eines speziellen Typs angreifen können führt das zu Unschärfen in der Vorhersage. Des weiteren wird die Position der Einheiten nicht mit einbezogen. 

\textcite{samvelyan2019starcraft} befasst sich mit den Problemen von Multi-Agent Reinforcement Learning (MARL). In dem Paper werden sogenannte Benchmark-Probleme für MARL eingeführt. Außerdem veröffentlichen die Verfasser mit \textit{PyMARL} ihr Framework (zu Deutsch etwa: \textit{Gerüst}) für die Erstellung und Analyse von tiefen MARL-Algorithmen. 

\textcite{Kilmer.1996} diskutiert in seinem Artikel die Verwendung von ANNs zur Vorhersage von Gefechtsausgängen in Militärszenarien. Es werden mehrere Anwendungsfälle skizziert in denen ANNs Beiträge leisten können, wie u.a. das Ersetzen bestehender Simulationsverfahren durch ANN-basierte Modelle, oder aber auch das Optimieren bestehender Verfahren durch Erkenntnisse aus ANN-Simulationen. 